#+TITLE: Incompressible Navier-Stokes

* Governing equations

This example solves the incompressible Navier-Stokes equations in a 3d
periodic unit box ([0,1]x[0,1]x[0,1]).  The governing equations are

  u_t + u·∇u = -∇p + μ ∇²u    subject to    ∇·u = 0.

In a 3d periodic domain, the pressure can be written as

  p = -Δ⁻¹( ∇·(u·∇u) )

and hence the governing equations become

  u_t = -u·∇u - ∇ ( -Δ⁻¹( ∇·(u·∇u) ) ) + μ ∇²u

for which the incompressiblity constraint is satisfied.

* Spatial discretization

The solver uses a pseudo-spectral spatial discretization.  Fourier
transforms are performed using the FFTW library.

The solver routines are meant to be as readable as possible, and
minimal effort has gone into making efficient use of the FFTW library.

Time stepping is done implicitly.

* Implicit solver

The implicit solver `impl_solve` in `feval.f90` uses a fixed point
iteration to solve

  U + a F(U) = RHS

for U, where F(U) = -U·∇U - ∇ ( -Δ⁻¹( ∇·(U·∇U) ) ) + μ ∇²U.  With this
kind of solver available a simple backward-Euler time-stepping given
by

  U(n+1) = U(n) + dt F[ U(n+1) ]

is trivial to implement with RHS=U(n) and a=-dt.  The implicit
midpoint rule given by

  U(n+1) = U(n) + dt F[ U(n)/2 + U(n+1)/2 ]

can be solved with the substitution U ← U(n)/2 + U(n+1)/2, RHS=U(n),
and a=-dt/2.

** Fixed point iteration

The fixed point iteration used to solve U + a F(U) = RHS proceeds as
follows:

1. Compute the non-linear and pressure terms using iteration k and
   move them to the RHS to obtain

     U(k+1) + a μ ∇²U(k+1) = RHS + a NL(k) + a ∇ ( -Δ⁻¹( ∇·NL(k) ) )

   where NL(k) = U(k)·∇U(k).

2. In Fourier space, the ∇² operator is easy to invert, and hence

     U(k+1) = (1 + a μ ∇²)⁻¹ [ RHS + a NL(k) + a ∇ p(k) ]

   where p(k) = -Δ⁻¹( ∇·NL(k) )

3. Iterate until |U(k+1) - U(k)| < tolerance.

* Taylor Green vortex behaviour

Using the implicit-midpoint (MP) rule, and running to a final time of
10.0

|    nu | npts |   dt | result             |
|-------+------+------+--------------------|
|  0.01 |   64 | 0.01 | all good           |
| 0.001 |   64 | 0.01 | blew up (step 650) |
| 0.001 |  128 | 0.01 | looks good         |
| 0.001 |  256 | 0.01 |                    |

With nu=0.001 an enstrophy of above 10 is probably bogus.

Using PFASST, and running to a final time of 10.0

|    nu |   dt | npts    | nnodes | nproc | niters | tol                  | result      |
|-------+------+---------+--------+-------+--------+----------------------+-------------|
| 0.001 | 0.01 | 64, 128 | 2, 3   |     4 |      4 | max(1.d-2**k, 1.d-9) | a bit wonky |
|       |      |         |        |       |        |                      |             |

It turns out that with a 256^3 grid, the enstrophy evolves
significantly differently from the 128^3 case (tested using MP).

Consider the following plot of enstrophies for nu=0.001
#+begin_src sh
python plot_enstrophy.py -o mp.pdf mp064.out mp128.out mp256.out
#+end_src
#+results:

[[file:mp.pdf]]

Running a 512^3 example doesn't seem feasible right now, unless we
move to a distributed memory solver (MPI FFTW).  As such, I'm going to
stick with 256^3 as the fine level and see how PFASST looks.

** Numerical experiments

Numerical experiments are contained in [[file:fabfile.py::taylor_green]].  To run the
experiments:

- modify source, commit, and push
- build

  #+begin_src sh
  fab build
  #+end_src

- stage runs

  #+begin_src sh
  fab taylor_green
  #+end_src

- on edison, submit

  #+begin_src sh
  cd /global/scratch2/sd/memmett/PFASST/tg
  qsub trial/submit.pbs
  #+end_src
